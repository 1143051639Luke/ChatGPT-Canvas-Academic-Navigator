0,1,2
"('page_content', 'Greedy Algorithms \nCPSC 320 2023W2')","('metadata', {'source': 'Preprocessing/Data/Origin/testpdf_1.pdf', 'page': 0})","('type', 'Document')"
"('page_content', 'Optimization problems \n●The next few worksheets will cover optimization problems :\n○We have a problem with several valid solutions \n○There is an objective function that tells us how good (or bad) each solution \nis\n○We are looking for the valid solution that minimizes or maximizes the value of \nthe objective function \n●We will look at two algorithm design paradigms: \n○Greedy algorithms \n○Dynamic programming')","('metadata', {'source': 'Preprocessing/Data/Origin/testpdf_1.pdf', 'page': 1})","('type', 'Document')"
"('page_content', 'Optimization problems \nExamples: \n●Given a set of intervals, find the largest set of intervals that don’t overlap. \n○The objective function is the cardinality of the set \n●Given a set of jobs with deadlines, order the jobs so as to minimize their \ntotal lateness \n○Objective function is the total lateness \n●Given a weighted connected graph, find a spanning tree with the smallest \ntotal edge weight \n○Objective function is the sum of the weights of the edges in the spanning tree')","('metadata', {'source': 'Preprocessing/Data/Origin/testpdf_1.pdf', 'page': 2})","('type', 'Document')"
"('page_content', 'Defining greedy algorithms \n●A greedy algorithm  proceeds by: \n○Making a choice based on a simple, local criterion \n○Solving the subproblem that results from that choice \n○Combining the choice and the subproblem choice \n●We can think of a greedy algorithm as making a sequence of \n(locally “good”) choices. \n●There is no precise definition of “greedy.”')","('metadata', {'source': 'Preprocessing/Data/Origin/testpdf_1.pdf', 'page': 3})","('type', 'Document')"
"('page_content', 'Defining greedy algorithms \nExamples of choice: \n●Choosing the interval with the earliest finish time. \n●Choosing the job with the earliest deadline. \n●Choosing the item needed further in the future. \n●Choosing the smallest weight edge that does not create a cycle.')","('metadata', {'source': 'Preprocessing/Data/Origin/testpdf_1.pdf', 'page': 4})","('type', 'Document')"
"('page_content', 'Defining greedy algorithms \nDoes a greedy algorithm always give the correct solution? \n●Sometimes yes, sometimes no. \n●There are some classes of problems (e.g., matroids) for which \nthere exists a greedy algorithm that always returns the correct \nsolution. \n●There are other problems where no one knows any greedy \nalgorithm with this property (e.g., weighted interval scheduling). \n●There are some problems where greedy doesn’t give an optimal \nsolution, but it’s provably *close* to optimal in some sense (e.g., \ntraveling salesperson).')","('metadata', {'source': 'Preprocessing/Data/Origin/testpdf_1.pdf', 'page': 5})","('type', 'Document')"
"('page_content', 'Proving a greedy algorithm correct \nMethod 1: “the greedy algorithm stays ahead” \n●Essentially a proof by induction \n●You compare the list of choices made by the greedy algorithm, to a \nsimilar list of choices made by an optimal solution \n●Show that at each stage, the greedy choice is at least as good as the \nchoice in the optimal solution \n●Example: algorithm for interval scheduling problem (4.1)')","('metadata', {'source': 'Preprocessing/Data/Origin/testpdf_1.pdf', 'page': 6})","('type', 'Document')"
"('page_content', 'Proving a greedy algorithm correct \nMethod 2: exchange arguments \n●Prove that if O is an optimal solution, and G is the greedy solution, then \nyou can modify O slightly to get O’ such that: \n○O’ is more similar to G than O was\n○O’ is at least as good a solution as O\n●Then describe how you can repeatedly modify O until it is the same \nsolution as G, without ever decreasing the solution quality \n●Examples of what “more similar to” might mean: \n○Has more edges in common with \n○Selects more of the same jobs \n○Has fewer elements out of order compared with the greedy solution')","('metadata', {'source': 'Preprocessing/Data/Origin/testpdf_1.pdf', 'page': 7})","('type', 'Document')"
"('page_content', 'Divide and Conquer \nAlgorithms \nCPSC 320 2023W2')","('metadata', {'source': 'Preprocessing/Data/Origin/testpdf_2.pdf', 'page': 0})","('type', 'Document')"
"('page_content', 'Definitions \nA divide and conquer algorithm proceeds by… \n●Dividing the input into two or more smaller instances of the \nsame problems \n○We call these subproblems \n●Solving the subproblems recursively \n●Combining the subproblem solutions to obtain a solution to \nthe original problem')","('metadata', {'source': 'Preprocessing/Data/Origin/testpdf_2.pdf', 'page': 1})","('type', 'Document')"
"('page_content', 'Examples \nSome divide and conquer algorithms you are already familiar \nwith:\n●QuickSort \n●MergeSort')","('metadata', {'source': 'Preprocessing/Data/Origin/testpdf_2.pdf', 'page': 2})","('type', 'Document')"
"('page_content', 'Recurrence relations \n●The running time T(n) of a recursive function can be described using \na recurrence relation: \n○T(n) is defined in terms of one or more terms of the form T(something smaller \nthan n) \n○Example:')","('metadata', {'source': 'Preprocessing/Data/Origin/testpdf_2.pdf', 'page': 3})","('type', 'Document')"
"('page_content', 'Recursion Trees \n●One way to solve a recurrence relation is to draw a recursion tree \n○Represent the recursion with a tree where each node represents a recursive \nsubproblem \n○Inside each node, write the size of the subproblem this call to the function \nsolves \n○Next to each node, write the amount of work done by the call to the function, \nnot including any time spent in recursive calls \n○Compute the total amount of non-recursive work done on each row \n○Then add up the work done over all rows')","('metadata', {'source': 'Preprocessing/Data/Origin/testpdf_2.pdf', 'page': 4})","('type', 'Document')"
"('page_content', 'Recursion trees')","('metadata', {'source': 'Preprocessing/Data/Origin/testpdf_2.pdf', 'page': 5})","('type', 'Document')"
"('page_content', 'Recursion trees')","('metadata', {'source': 'Preprocessing/Data/Origin/testpdf_2.pdf', 'page': 6})","('type', 'Document')"
"('page_content', 'Recursion trees')","('metadata', {'source': 'Preprocessing/Data/Origin/testpdf_2.pdf', 'page': 7})","('type', 'Document')"
"('page_content', 'Recursion trees')","('metadata', {'source': 'Preprocessing/Data/Origin/testpdf_2.pdf', 'page': 8})","('type', 'Document')"
"('page_content', 'Recursion trees')","('metadata', {'source': 'Preprocessing/Data/Origin/testpdf_2.pdf', 'page': 9})","('type', 'Document')"
"('page_content', 'Recursion trees')","('metadata', {'source': 'Preprocessing/Data/Origin/testpdf_2.pdf', 'page': 10})","('type', 'Document')"
"('page_content', 'Recursion trees')","('metadata', {'source': 'Preprocessing/Data/Origin/testpdf_2.pdf', 'page': 11})","('type', 'Document')"
"('page_content', 'Recursion trees')","('metadata', {'source': 'Preprocessing/Data/Origin/testpdf_2.pdf', 'page': 12})","('type', 'Document')"
"('page_content', 'Recursion trees')","('metadata', {'source': 'Preprocessing/Data/Origin/testpdf_2.pdf', 'page': 13})","('type', 'Document')"
"('page_content', 'Recursion trees')","('metadata', {'source': 'Preprocessing/Data/Origin/testpdf_2.pdf', 'page': 14})","('type', 'Document')"
"('page_content', 'Recursion trees')","('metadata', {'source': 'Preprocessing/Data/Origin/testpdf_2.pdf', 'page': 15})","('type', 'Document')"
"('page_content', 'Recursion trees')","('metadata', {'source': 'Preprocessing/Data/Origin/testpdf_2.pdf', 'page': 16})","('type', 'Document')"
"('page_content', 'The Master Theorem \n●Most divide and conquer algorithms split the input into \nequal-size subproblems \n●Most recursion trees fall into one of three categories: \n○The work per level increases geometrically \n○The work per level is constant (e.g., MergeSort) \n○The work per level decreases geometrically (e.g., the previous \nexample)')","('metadata', {'source': 'Preprocessing/Data/Origin/testpdf_2.pdf', 'page': 17})","('type', 'Document')"
"('page_content', 'The Master Theorem [Bentley, Haken, Saxe]')","('metadata', {'source': 'Preprocessing/Data/Origin/testpdf_2.pdf', 'page': 18})","('type', 'Document')"
"('page_content', 'The Master Theorem [Bentley, Haken, and Saxe] \nHow to apply the theorem: \n●Compute logba\n●Compare it to the exponent of n in f(n)\n○If logba is larger: case 1. \n○If they are equal: maybe case 2. \n○If logba is smaller: check regularity condition, and if it holds then case \n3.')","('metadata', {'source': 'Preprocessing/Data/Origin/testpdf_2.pdf', 'page': 19})","('type', 'Document')"
